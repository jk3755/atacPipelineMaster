########################################################################################################################################
#### NOTES #############################################################################################################################
########################################################################################################################################
## Spool the pipeline with the following parameters:
# snakemake
# --snakefile snakefileATACseqWorkflow.snakefile
# -j 20 
# [rule]
# --resources mem_mb=50000
# --use-conda
# --restart-times 3
#
## Parameters:
# --snakefile: specify the file where the snakemake workflow is contained
# j: specifies the number of threads the run will use
# restart-times: sets the number of times snakemake will attempt to restart a failed job
# --rerun-incomplete: can be used to regenerate incomple files
# --unlock: unlocks the snakemake working directory, such as after a power loss or kill signal
#
## Resource definitions
# mem_mb: specifies the total memory limit of the pipeline. This relies on mem_mb being user defined in individual rules
#
## Cluster configuration
# When running snakemake workflows on a cluster, initiate an interactivate session from which you will run the pipeline
# Note, limits seem to be weird on interactive, cant seem to initiate more than 6 hours?
# qlogin -l mem=4G,time=8:0:0
# make sure to cd to the appropriate directory
# for example, /ifs/scratch/c2b2/ac_lab/jk3755/atac/
# Activate the appropriate conda environment
# module load conda
# source activate atac
# you can then use a command similar to this to execute the workflow from the interactive session
# snakemake --snakefile snakefileATACseqWorkflow.snakefile -j 1000 rawFP_lncap --cluster-config qsubConfig.json
# --cluster "qsub -cwd -pe smp {cluster.nCPUs} -l mem={cluster.memory}M,time={cluster.runtime}:0:0 -V" --use-conda --restart-times 5 --latency-wait 60
#
## Options that may be useful when running
# --list-conda-envs - lists conda envs and their locations
# --cleanup-conda - cleans up unused conda envs
# --create-envs-only - allows you to run the pipeline but only create the conda envs necessary (first time runs)
# --verbose - set verbose output
# --debug-dag - print the dag workflow
# --printshellcmds - print shell commands that will be executed
# --reason - print the reason for execution of each rule

########################################################################################################################################
#### RULE PARAMETERS ###################################################################################################################
########################################################################################################################################
Build dirs
## -p ignore error if existing
## -v verbose

## 
rule STEP1_gunzip:
    # -k keep original files
    # -c write to standard output

    ## Fastp fastq QC Filtering
rule STEP2_fastp_filtering:
    # -i, -I specify paired end input
    # -o, -O specifies paired end output
    # -w specifies the number of threads to use (16 max)
    # -h specifies output for the html QC report
    # -j specifies the json output QC report

    rule STEP3_mycoalign:
    # -q fastq input file format
    # -p num threads to use
    # -X1000 align to a maximum of 2000 bp frag length
    # -1 is read 1 input fastq file
    # -2 is read 2 input fastq file
    # -S output file path
    # 2> bowtie2 outputs alignment metrics to STDERR, 2> will allow redirect to a text file

rule STEP4_hg38align:
    # use 'snakemake --resources hg38align=1' to limit the number of parallel instances of this rule
    # -q fastq input file format
    # -p num threads to use
    # -X1000 align to a maximum of 2000 bp frag length
    # -1 is read 1 input fastq file
    # -2 is read 2 input fastq file
    # -S output file path
    # 2> bowtie2 outputs alignment metrics to STDERR, 2> will allow redirect to a text file

 rule STEP5_coordsort_sam:
    # -o output file path
    # -O output file format

rule STEP6_blacklistfilter_bamconversion:
    # -b output in bam format
    # -h include header in output file
    # -o specify output file path
    # -L only output alignments that overlap with the provided BED file
    # -U write the alignments NOT selected by other parameters to the specified file
    # -@ specify number of threads

rule STEP7_chrM_contamination:
    # remove mitochondrial reads
    # params:
    # -b input file is in bam format
    # -h keep the sam header. important downstream
    # -o output filepath for reads NOT matching to blacklist region
    # -L path to the blacklist BED file
    # -U output filepath for reads matching blacklist region
    # -@ number of threads to use

rule STEP8_addrgandcsbam:
    # refer to https://software.broadinstitute.org/gatk/documentation/article.php?id=6472 for information on read group tags
    # note - proper specification of RG tags is critical for downstream analysis and unique sample identification when submitting for publication
    # specification of the lane allows optical duplicates to be detected(?)
    # required by GATK standards
    # also important for identifying batch effects/technical artifacts(?)
    # see: https://software.broadinstitute.org/gatk/documentation/article.php?id=6472
    # Required @RG parameter specifications:
    # RGID (read group ID) - this must be a globally unique string. for illumina data, use flowcell + lane
    # RGLB (read group library) - This is used by MarkDuplicates to collect reads from the same library on different lanes, so it must be common to all files from the same library
    # RGPL (read group platform) - ILLUMINA
    # RGPU (read group platform unit) - The PU holds three types of information, the {FLOWCELL_BARCODE}.{LANE}.{SAMPLE_BARCODE}
    # RGSM (read group sample name) - the name of the sample sequenced in this file. should be consistent across different files from different lanes
    # params:
    # I specifies the input file
    # O specifies the output file

rule STEP9_cleansam:
    # soft-clips bases aligned past the end of the ref sequence
    # soft-clipping retains the bases in the SEQ string, but they are not displayed or used in downstream data analysis
    # sets MAPQ score to 0 for unmapped reads
    # I specifies the input file
    # O specifies the output file

rule STEP10_mergelanes:
    # Merge files for individual lanes
    # I specifies input files for each lane
    # O specifies the output files
    # SORT_ORDER/ASSUME_SORTED specify the type of sorting in the input files
    # MERGE_SEQUENCE_DICTIONARIES will combine the sequence dictionaries from the individual files
    # a sequence dictionary contains information about sequence name, length, genome assembly ID, etc
    # USE_THREADING allows multithreadded operation for the bam compression

rule STEP10b_clean_intermediate_data:
    # -rm removes files
    # -f forces the removal

rule STEP11_purgeduplicates:
	# -Xmx50g specifies a 50 gb memory limit per process
    # I specifies the input file
    # O specifies the output file
    # M specifies the duplication metrics output file
    # REMOVE_DUPLICATES enables removal of duplicate reads from the output file
    # ASSUME_SORTED indicates the input file is already sorted